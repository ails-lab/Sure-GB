{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A complete example of the proposed pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[nltk_data] Downloading package omw-1.4 to /home/geofila/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "/data/data1/users/geofila/Anaconda3/envs/myenv/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /data/data1/users/geofila/Anaconda3/envs/myenv/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "[nltk_data] Downloading package omw-1.4 to /home/geofila/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: sentence-transformers/all-mpnet-base-v2\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device: cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a30a638d65ad47d28976702336ae19f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No chat template is set for this tokenizer, falling back to a default class-level template. This is very error-prone, because models are often trained with templates different from the class default! Default chat templates are a legacy feature and will be removed in Transformers v4.43, at which point any code depending on them will stop working. We recommend setting a valid chat template before then to ensure that this model continues working without issues.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bc72c13e873476ca03dbed544066c36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b56ffc84e090457498a0f75b4c2fa387",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7b77d96c8ef401ebf91faea15436957",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d833639ce9a74d7ba995766d9019b106",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from occ_models import HF_Model, find_occupations_aws\n",
    "from utils import extract_occupations_from_resp, Knowledge\n",
    "from gender import * \n",
    "import os \n",
    "\n",
    "\n",
    "## English\n",
    "text_source = \"A lawyer and a butcher.\"\n",
    "language_source = \"en\"\n",
    "\n",
    "### Greek\n",
    "text_target = \"Ένας δικηγόρος και ένας χασάπης.\"\n",
    "language_target = \"el\"\n",
    "\n",
    "### French\n",
    "# text = \"C'est un boucher\"\n",
    "# language = \"fr\"\n",
    "\n",
    "\n",
    "#### Setting up the Pipeline #####\n",
    "el = Greek()\n",
    "en = English()\n",
    "fr = French()\n",
    "\n",
    "# Create the Knowledge\n",
    "knowledge = Knowledge(\"ISCO-08-EN.csv\", column = \"Definition\")\n",
    "\n",
    "\n",
    "# model = HF_Model()\n",
    "# model.find_occupations(text)\n",
    "\n",
    "os.environ['aws_access_key_id'] = '...'\n",
    "os.environ['aws_secret_access_key'] = '...'\n",
    "#######\n",
    "\n",
    "\n",
    "def extract_occupations_and_gender(text, language, knowledge):\n",
    "    # Step 1: Find Occupations using LLM\n",
    "    llm_resp = find_occupations_aws(text)\n",
    "    # Step 2: Analyze output of LLM to extract occupations and definitions if are existed\n",
    "    list_of_occs = extract_occupations_from_resp(llm_resp)\n",
    "\n",
    "    responses = []\n",
    "    for row in list_of_occs:\n",
    "        index, p = knowledge.connect(row['definition'])[0]\n",
    "\n",
    "\n",
    "        if language == \"el\":\n",
    "            check_coreference = False\n",
    "            nlp = el.nlp\n",
    "        if language == \"fr\":\n",
    "            check_coreference = True\n",
    "            nlp = fr.nlp\n",
    "        if language == \"en\":\n",
    "            check_coreference = True\n",
    "            nlp = en.nlp\n",
    "\n",
    "        gender = find_gender(nlp, text, [row], check_coreference = check_coreference)\n",
    "\n",
    "        responses.append({\n",
    "                \"index\": index,\n",
    "                \"title\": row['title'],\n",
    "                \"p\": round (p*100, 2),\n",
    "                \"kg\": knowledge.describe_occ_dict(index),\n",
    "                \"gender\": gender,\n",
    "                \"text\": text,\n",
    "                \"language\": language\n",
    "\n",
    "            })\n",
    "\n",
    "    return responses\n",
    "\n",
    "resp_source = extract_occupations_and_gender(text_source, language_source, knowledge)\n",
    "resp_target = extract_occupations_and_gender(text_target, language_target, knowledge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import itertools\n",
    "\n",
    "model = transformers.BertModel.from_pretrained('bert-base-multilingual-cased')\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-multilingual-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_word_start_end(src, token_index):\n",
    "    words = src.strip().split()\n",
    "    current_position = 0\n",
    "    \n",
    "    for i, word in enumerate(words):\n",
    "        start_position = current_position\n",
    "        end_position = start_position + len(word) - 1\n",
    "        \n",
    "        if i == token_index:\n",
    "            return [start_position, end_position + 1]\n",
    "        \n",
    "        current_position = end_position + 2  # +1 for the space, +1 because end_position is inclusive\n",
    "\n",
    "\n",
    "def align(src, tgt, ds, dt, model, tokenizer):\n",
    "#     sent_src, sent_tgt = src.strip().split(), tgt.strip().split()\n",
    "    sent_src, sent_tgt = [str(s) for s in ds.nlp(src)], [str(s) for s in dt.nlp(tgt)]\n",
    "    token_src, token_tgt = [tokenizer.tokenize(word) for word in sent_src], [tokenizer.tokenize(word) for word in sent_tgt]\n",
    "    wid_src, wid_tgt = [tokenizer.convert_tokens_to_ids(x) for x in token_src], [tokenizer.convert_tokens_to_ids(x) for x in token_tgt]\n",
    "    ids_src, ids_tgt = tokenizer.prepare_for_model(list(itertools.chain(*wid_src)), return_tensors='pt', model_max_length=tokenizer.model_max_length, truncation=True)['input_ids'], tokenizer.prepare_for_model(list(itertools.chain(*wid_tgt)), return_tensors='pt', truncation=True, model_max_length=tokenizer.model_max_length)['input_ids']\n",
    "    sub2word_map_src = []\n",
    "    for i, word_list in enumerate(token_src):\n",
    "        sub2word_map_src += [i for x in word_list]\n",
    "        sub2word_map_tgt = []\n",
    "    for i, word_list in enumerate(token_tgt):\n",
    "        sub2word_map_tgt += [i for x in word_list]\n",
    "\n",
    "    # alignment\n",
    "    align_layer = 8\n",
    "    threshold = 1e-3\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out_src = model(ids_src.unsqueeze(0), output_hidden_states=True)[2][align_layer][0, 1:-1]\n",
    "        out_tgt = model(ids_tgt.unsqueeze(0), output_hidden_states=True)[2][align_layer][0, 1:-1]\n",
    "\n",
    "        dot_prod = torch.matmul(out_src, out_tgt.transpose(-1, -2))\n",
    "\n",
    "        softmax_srctgt = torch.nn.Softmax(dim=-1)(dot_prod)\n",
    "        softmax_tgtsrc = torch.nn.Softmax(dim=-2)(dot_prod)\n",
    "\n",
    "        softmax_inter = (softmax_srctgt > threshold)*(softmax_tgtsrc > threshold)\n",
    "\n",
    "    align_subwords = torch.nonzero(softmax_inter, as_tuple=False)\n",
    "    align_words = set()\n",
    "    for i, j in align_subwords:\n",
    "        align_words.add( (sub2word_map_src[i], sub2word_map_tgt[j]) )\n",
    "\n",
    "    al = {}\n",
    "    for i, j in sorted(align_words):\n",
    "        al[i] = [j, sent_src[i], sent_tgt[j]]\n",
    "\n",
    "    return al\n",
    "\n",
    "def search_g(resp_target, m):\n",
    "    for row2 in resp_target:\n",
    "        for r2 in row2[\"gender\"]:\n",
    "            s2, e2 = r2[\"tokens\"]\n",
    "            g2 = r2[\"gender\"]\n",
    "            if s2 == m:\n",
    "                return row2[\"title\"], g2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found gender shift in the word: lawyer → δικηγόρος, from NC to Masc\n",
      "Found gender shift in the word: butcher → χασάπης, from NC to Masc\n"
     ]
    }
   ],
   "source": [
    "mapping = align(text_source, text_target, en, el, model, tokenizer)\n",
    "for row1 in resp_source:\n",
    "    for r1 in row1[\"gender\"]:\n",
    "        s1, e1 = r1[\"tokens\"]\n",
    "        g1 = r1[\"gender\"]\n",
    "        \n",
    "        t2, g2 = search_g(resp_target, mapping[s1][0])\n",
    "                \n",
    "        if g1 != g2:\n",
    "            print (f\"Found gender shift in the word: {row1['title']} → {t2}, from {g1} to {g2}\")\n",
    "                "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv_3.10.12",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
